{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8437d4d2",
   "metadata": {},
   "source": [
    "# Готовим тексты на русском для Machine Learning с Python\n",
    "\n",
    "[https://python-school.ru/blog/russian-text-preprocessing/?ysclid=l5r25d3ldg544162530](https://python-school.ru/blog/russian-text-preprocessing/?ysclid=l5r25d3ldg544162530)\n",
    "\n",
    "Автор [Роман Котюбеев](https://python-school.ru/blog/author/favorshlitex/)\n",
    "\n",
    "![](https://python-school.ru/wp-content/uploads/2020/07/untitled-diagram-1.png)\n",
    "\n",
    "В [прошлый раз](https://python-school.ru/blog/nlp-text-preprocessing/) мы разобрали, как обработать текстовые данные с помощью разных Python-библиотек. Сегодня мы расскажем, как с помощью Python подготовить настоящий датасет с разнообразными twitter-постами на русском языке перед созданием модели Machine Learning.\n",
    "\n",
    "## Модель Word2vec на основе датасета русскоязычных twitter-постов\n",
    "\n",
    "В качестве примера построим модель Word2vec, обученную на русскоязычном корпусе коротких текстов RuTweetCorp, который собран Ю.Рубцовой \\[1\\]. Корпус разбит на два класса: твиты с положительной и с отрицательной окрасками. Для обработки нам понадобится всего две библиотеки:\n",
    "\n",
    "- Pymorphy2 для лемматизации \\[2\\].\n",
    "- NLTK, который имеет список стоп-слов \\[3\\].\n",
    "\n",
    "Прежде всего требуется скачать корпус, как отрицательных, так и положительных твитов, в формате CSV (Comma separated-values) вот здесь. После установки обеих Python-библиотек необходимо получить стоп-слова от NLTK. Это можно сделать, написав в среде разработке, например, Jupyter Notebook, следующее:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "## Соединяем файлы с негативными и позитивными twitter-постами в один большой датасет\n",
    "\n",
    "Как уже говорилось, корпус разбит на два класса под названием negative.csv и positive.csv. Прочитаем файлы через Python-библиотеку Pandas. Заметим, что данные разделены не запятой, а двоеточием, поэтому ставим аргумент sep=\";\":\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df_pos = pd.read_csv(\"/content/positive.csv\", sep=\";\", header=None)\n",
    "df_neg = pd.read_csv(\"/content/negative.csv\", sep=\";\", header=None)\n",
    "```\n",
    "\n",
    "Оба файла имеет множество столбцов, таких как имя автора поста, дата публикации и т.д., но нас интересует только сами twitter-посты. Индекс столбца с постами стоит под номером 3; посмотрим, какие тексты содержаться в корпусе с позитивными постами:\n",
    "\n",
    "```\n",
    ">>> df1.iloc[:, 3]\n",
    "0      @first_timee хоть я и школота, но поверь, у на...\n",
    "1      Да, все-таки он немного похож на него. Но мой ...\n",
    "2      RT @KatiaCheh: Ну ты идиотка) я испугалась за ...\n",
    "3      RT @digger2912: \"Кто то в углу сидит и погибае...\n",
    "4      @irina_dyshkant Вот что значит страшилка :D\\nН...\n",
    "...\n",
    "114906 Спала в родительском доме, на своей кровати......\n",
    "114907 RT @jebesilofyt: Эх... Мы немного решили сокра...\n",
    "114908 Что происходит со мной, когда в эфире #proacti...\n",
    "114909 \"Любимая,я подарю тебе эту звезду...\" Имя како...\n",
    "114910 @Ma_che_rie посмотри #непытайтесьпокинутьомск ...\n",
    "````\n",
    "И тот, и другой файл содержат чуть более 100000 записей. Поскольку для нас не имеет значения позитивные или негативные посты (мы строим общую картину), соединим оба файла в один, отбросив пустые записи и дубликаты:\n",
    "\n",
    "```python\n",
    "df = df_pos.iloc[:, 3].append(df_neg.iloc[:, 3])\n",
    "df = df.dropna().drop_duplicates()\n",
    "```\n",
    "\n",
    "## Проводим токенизацю, лемматизацию и удаляем стоп-слова\n",
    "\n",
    "Как уже было рассказано [тут](https://python-school.ru/nlp-text-preprocessing/), прежде в работе с [NLP](https://python-school.ru/wiki/nlp/)-задачами требуется нормализовать данные. В данном случае, мы проведем лемматизацию и удалим стоп-слова, воспользовавшись Python-библиотеками pymorphy2 и [NLTK](https://python-school.ru/wiki/nltk/) соответственно.\n",
    "\n",
    "Мы уже создали DataFramе из двух файлов, поэтому просто используем метод `apply`, который применяет пользовательскую функцию к каждой записи DataFrame. Эту функцию мы и создадим, в которой:\n",
    "\n",
    "1. избавимся от букв латинского алфавита, чисел, знаков препинания и всех символов, например, символ @ встречается почти везде;\n",
    "2. разобьем пост на токены;\n",
    "3. проведем лемматизацияю, получив нормальную (начальную) форму слова;\n",
    "4. удалим стоп-слова.\n",
    "\n",
    "Вот эта функция `lemmatize` сделает все вышеперечисленное:\n",
    "\n",
    "```python\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "patterns = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "stopwords_ru = stopwords.words(\"russian\")\n",
    "morph = MorphAnalyzer()\n",
    "def lemmatize(doc):\n",
    "    doc = re.sub(patterns, ' ', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token and token not in stopwords_ru:\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            \n",
    "            tokens.append(token)\n",
    "    if len(tokens) > 2:\n",
    "        return tokens\n",
    "    return None\n",
    "```\n",
    "\n",
    "Стоит обратить внимание, что переменные `patterns`, `stopwords_ru` и `morph` инициализированы вне функции `lemmatize`, так как создавать их для каждой записи не имеет смысла — они не изменяются. Переменная `patterns` нужна чтобы избавиться от всех некириллических символов и используется в функции `re.sub`. Переменная `morph` – это морфологический анализатор, который используется для нахождения нормальной формы слова. Функция `lemmatize` возвращает список слов (токенов), причем только тех постов, который содержат более 2 слов.\n",
    "\n",
    "Остается только применить функцию к объекту DataFrame:\n",
    "\n",
    "```python\n",
    "data = df.apply(lemmatize)\n",
    "```\n",
    "\n",
    "Обработка в Google Colab занимает около 5 минут, то есть примерно 40000 записей за 1 минуту. Можем посмотреть на результат:\n",
    "\n",
    "```\n",
    ">>> data = data.dropna()\n",
    ">>> data\n",
    "0         [школотый, поверь, самый, общество, профилиров...\n",
    "1          [да, таки, немного, похожий, но, мальчик, равно]\n",
    "2                                 [ну, идиотка, испугаться]\n",
    "3         [кто, угол, сидеть, погибать, голод, ещё, порц...\n",
    "4         [вот, значит, страшилка, но, блин, посмотреть,...\n",
    "...\n",
    "205174                     [но, каждый, хотеть, исправлять]\n",
    "205175          [скучать, вправлять, мозги, равно, скучать]\n",
    "205176                       [вот, школа, говно, это, идти]\n",
    "205177                           [тауриэль, грусть, обнять]\n",
    "205178    [такси, везти, работа, раздумывать, приплатить...\n",
    "```\n",
    "\n",
    "## Самые распространенные слова в датасете\n",
    "\n",
    "Есть возможность также посмотреть на самые распространенные слова во все обработанном датасете. Для этого воспользуемся особым словарем — `defaultdict`. В случае если ключ отсутствует в этом словаре, то ему присваивается значение по умолчанию, соответствующее переданному типу данных: значение по умолчанию для int — 0, для float — 0.0, для str — “” и т.д. От нас требуется всего лишь пройтись по токенам датасета:\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "word_freq = defaultdict(int)\n",
    "for tokens in data.iloc[:]:\n",
    "    for token in tokens:\n",
    "        word_freq[token] += 1\n",
    "```        \n",
    "Можем взглянуть сколько уникальных слов и 10 самых повторяющихся слов:\n",
    "\n",
    "```\n",
    ">>> len(word_freq)\n",
    "98237\n",
    ">>> sorted(word_freq, key=word_freq.get, reverse=True)[:10]\n",
    "['это', 'я', 'весь', 'хотеть', 'день', 'ты', 'такой', 'а', 'сегодня', 'быть']\n",
    "```\n",
    "\n",
    "Как видим, самые распространенные слова оказались не самыми информативными.\n",
    "\n",
    "***\n",
    "\n",
    "В [следующей статье](https://python-school.ru/word2vec-with-examples-in-gensim/) расскажем, как на полученном датасете обучить модель Word2Vec с помощью Python-библиотеки Gensim. А как подготовить данные для Machine Learning в реальных примерах Data Science, вы узнаете на наших курсах в лицензированном учебном центре обучения и повышения квалификации ИТ-специалистов в Москве.\n",
    "\n",
    "## Источники\n",
    "1. Рубцова Ю. Автоматическое построение и анализ корпуса коротких текстов (постов микроблогов) для задачи разработки и тренировки тонового классификатора //Инженерия знаний и технологии семантического веба. – 2012. – Т. 1. – С. 109-116.\n",
    "2. [https://pymorphyreadthedocs.io/en/latest/index.html](https://pymorphyreadthedocs.io/en/latest/index.html)\n",
    "3. [https://www.nltk.org/](https://www.nltk.org/)\n",
    "\n",
    "# Обучение NLP-модели Word2veс на русских текстах с Python\n",
    "\n",
    "[https://python-school.ru/blog/word2vec-with-examples-in-gensim/?ysclid=l5r2cu50a8657126615](https://python-school.ru/blog/word2vec-with-examples-in-gensim/?ysclid=l5r2cu50a8657126615)\n",
    "\n",
    "Автор [Роман Котюбеев](https://python-school.ru/blog/author/favorshlitex/)\n",
    "\n",
    "![](https://python-school.ru/wp-content/uploads/2020/07/1-2.png)\n",
    "\n",
    "Продолжаем решать [NLP](https://python-school.ru/wiki/nlp/)-задачи на примере корпуса с русскоязычными twitter-постами, на основе которого мы получили датасет \\[[вот здесь](https://python-school.ru/?p=5384&preview=true)\\]. Сегодня мы расскажем, как построить и обучить свою word2vec-модель [Machine Learning](https://python-school.ru/wiki/machine-learning/), используя Python-библиотеку Gensim.\n",
    "\n",
    "## Модель Word2vec на основе датасета русскоязычных twitter-постов\n",
    "\n",
    "В предыдущей статье мы подготовили датасет: провели лемматизацию и удалили стоп слова из корпуса, который содержит twitter-посты на русском языке \\[1\\]. Вот так он сейчас выглядят:\n",
    "\n",
    "```\n",
    ">>> data\n",
    "0         [школотый, поверь, самый, общество, профилиров...\n",
    "1          [да, таки, немного, похожий, но, мальчик, равно]\n",
    "2                                 [ну, идиотка, испугаться]\n",
    "3         [кто, угол, сидеть, погибать, голод, ещё, порц...\n",
    "4         [вот, значит, страшилка, но, блин, посмотреть,...\n",
    "...\n",
    "205174                     [но, каждый, хотеть, исправлять]\n",
    "205175          [скучать, вправлять, мозги, равно, скучать]\n",
    "205176                       [вот, школа, говно, это, идти]\n",
    "205177                           [тауриэль, грусть, обнять]\n",
    "205178    [такси, везти, работа, раздумывать, приплатить...\n",
    "```\n",
    "\n",
    "Далее построим модель Word2vec, обученную на полученном датасете, с библиотекой Gensim \\[2\\].\n",
    "\n",
    "## Обучение модели Word2vec\n",
    "\n",
    "Подготовив датасет, можем обучить модель. Для этого воспользуемся библиотекой Gensim и инициализируем модель Word2vec:\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(\n",
    "    min_count=10,\n",
    "    window=2,\n",
    "    size=300,\n",
    "    negative=10,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    sample=6e-5,\n",
    "    sg=1)\n",
    "```    \n",
    "Модель имеет множество аргументов:\n",
    "\n",
    "- `min_count` — игнорировать все слова с частотой встречаемости меньше, чем это значение.\n",
    "- `windоw` — размер контекстного окна, о котором говорили [тут](https://python-school.ru/nlp-text-preprocessing/), обозначает диапазон контекста.\n",
    "- `size` — размер векторного представления слова (word embedding).\n",
    "- `negative` — сколько неконтекстных слов учитывать в обучении, используя negative sampling, о нем также упоминалось [здесь](https://python-school.ru/nlp-text-preprocessing/).\n",
    "- `alpha` — начальный learning_rate, используемый в алгоритме обратного распространения ошибки (Backpropogation).\n",
    "- `min_alpha` — минимальное значение learning_rate, на которое может опуститься в процессе обучения.\n",
    "- `sg` — если 1, то используется реализация Skip-gram; если 0, то CBOW. О реализациях также говорили [тут](https://python-school.ru/what-is-word2vec/).\n",
    "\n",
    "Далее, требуется получить словарь:\n",
    "\n",
    "```python\n",
    "w2v_model.build_vocab(data)\n",
    "```\n",
    "А после уже можно обучить модель, используя метод train:\n",
    "\n",
    "```python\n",
    "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "```\n",
    "\n",
    "Если в дальнейшем не требуется снова обучать модель, то для сохранения оперативной памяти можно написать следующее:\n",
    "\n",
    "```python\n",
    "w2v_model.init_sims(replace=True)\n",
    "```\n",
    "\n",
    "## Насколько похожи слова обученной модели Word2vec\n",
    "\n",
    "После того, как модель была обучена, можем смотреть результаты. Каждое слово представляется вектором, следовательно, их можно сравнивать. В качестве инструмента сравнения в Gensim используется косинусный коэффициент (Cosine similarity) \\[3\\].\n",
    "\n",
    "У модели Word2vec имеется в качестве атрибута объект wv, который и содержит векторное представление слов (word embeddings). У этого объекта есть методы для получения мер схожестей слов. Например, определим, какие слова находятся ближе всего к слову “любить”:\n",
    "\n",
    "```\n",
    ">>> w2v_model.wv.most_similar(positive=[\"любить\"])\n",
    "[('дорожить', 0.5577003359794617),\n",
    "('скучать', 0.4815309941768646),\n",
    "('обожать', 0.477267324924469),\n",
    "('машенька', 0.4503161907196045),\n",
    "('предновогодниеобнимашка', 0.4403109550476074),\n",
    "('викуль', 0.43941542506217957),]\n",
    "```\n",
    "\n",
    "Число после запятой обозначает косинусный коэффициент, чем он больше, тем выше близость слов. Можно заметить, что слова “дорожить”, “скучать”, “обожать” наиболее близкие к слову “любить” Можно также рассмотреть другие слова:\n",
    "\n",
    "```\n",
    ">>> w2v_model.wv.most_similar(positive=[\"мужчина\"])\n",
    "[('женщина', 0.6242121458053589),\n",
    "('девушка', 0.5410279035568237),\n",
    "('любящий', 0.5005632638931274),\n",
    "('парень', 0.4864271283149719),\n",
    "('идеал', 0.45188209414482117),\n",
    "('существо', 0.44532185792922974),\n",
    "('недостаток', 0.4350862503051758),\n",
    "('пёсик', 0.42453521490097046),\n",
    "('послушный', 0.42428380250930786),\n",
    "('эгоизм', 0.4202057421207428)]\n",
    "...\n",
    ">>> w2v_model.wv.most_similar(positive=[\"день\", \"завтра\"])\n",
    "[('сегодня', 0.6082668304443359),\n",
    "('неделя', 0.5371285676956177),\n",
    "('суббота', 0.48631012439727783),\n",
    "('выходной', 0.4772387742996216),\n",
    "('понедельник', 0.4697558283805847),\n",
    "('денёчек', 0.4688040316104889),\n",
    "('каникулы', 0.45828908681869507),\n",
    "('подъесть', 0.4555707573890686),\n",
    "('отсыпаться', 0.44570696353912354),\n",
    "('аттестация', 0.4408838450908661)]\n",
    "```\n",
    "Векторы можно складывать и вычитать. Например, рассмотрим такой вариант: “папа” + “брат” — “мама”. Получили следующее:\n",
    "\n",
    "```\n",
    ">>> w2v_model.wv.most_similar(positive=[\"папа\", \"брат\"], negative=[\"мама\"])\n",
    "[('младший', 0.3892076015472412),\n",
    "('сестра', 0.31560415029525757),\n",
    "('двоюродный', 0.3024488091468811),\n",
    "('старший', 0.2937452793121338),]\n",
    "```\n",
    "Как видим, на этом корпусе результатом являются слова “младший”, “сестра”, “двоюродный”. Несмотря на то, что мы обучили модель на twitter-постах, получаем достаточно адекватные результаты.\n",
    "\n",
    "Есть также возможность определить наиболее близкое слово из списка к данному слово. Для этого нужно воспользоваться методом:\n",
    "\n",
    "```\n",
    ">>> w2v_model.wv.most_similar_to_given(\"хороший\", [\"приятно\", \"город\", \"мальчик\"])\n",
    "'приятно'\n",
    "```\n",
    "\n",
    "Слово “приятно” из всего списка наиболее близок к слову “хороший”. Кроме того, так как для сравнения находится косинусный коэффициент, то его можно получить, написав:\n",
    "\n",
    "```\n",
    ">>> w2v_model.wv.similarity(\"плохой\", \"хороший\")\n",
    "0.5427995\n",
    ">>> w2v_model.wv.similarity(\"плохой\", \"герой\")\n",
    "0.04865976\n",
    "```\n",
    "Само векторное представление слова можно получит либо напрямую, обратившись через квадратные скобки, либо через метод word_vec:\n",
    "\n",
    "```\n",
    ">>> w2v_model.wv.word_vec(\"страшилка\")\n",
    "array([-0.05101333, -0.03730767,  0.03676478,  0.18950877,  0.02496774,\n",
    "        0.00176699, -0.0966768 ,  0.04010197, -0.00862965, -0.00530563,\n",
    "...\n",
    "dtype=float32)\n",
    ">>> w2v_model.wv[\"страшилка\"]\n",
    "array([-0.05101333, -0.03730767,  0.03676478,  0.18950877,  0.02496774,\n",
    "        0.00176699, -0.0966768 ,  0.04010197, -0.00862965, -0.00530563,\n",
    "...\n",
    "dtype=float32)\n",
    ">>> w2v_model.wv.word_vec(\"страшилка\").shape\n",
    "(300,)\n",
    "```\n",
    "\n",
    "## Векторное представление слов на плоскости\n",
    "\n",
    "Так как слова в модели Word2vec — это векторы, то можно их выстроить на координатную плоскость. Поскольку каждый вектор модели имеет размерность 300, который был указан как аргумент size, то не предоставляется возможности построить 300-мерное пространство. Поэтому мы воспользуемся методом уменьшения размерности t-SNE, который есть в Python-библиотеке Scikit-learn \\[4\\], и снизим размерность векторов с 300 до 2. Согласно документации, если размерность больше 50, то стоит сначала воспользоваться другим методом уменьшения размерности — методом главных компонент PCA [5]. Для этого в аргументе достаточно указать `init=”pca”`.\n",
    "\n",
    "В результате мы написали функцию, которая принимает входное слово и список слов и строит на плоскости входное слово, ближайшие к нему слова и переданный список слов:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "def tsne_scatterplot(model, word, list_names):\n",
    "    \"\"\"Plot in seaborn the results from the t-SNE dimensionality reduction \n",
    "    algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\"\"\"\n",
    "    vectors_words = [model.wv.word_vec(word)]\n",
    "    word_labels = [word]\n",
    "    color_list = ['red']\n",
    "    close_words = model.wv.most_similar(word)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.word_vec(wrd_score[0])\n",
    "        vectors_words.append(wrd_vector)\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.word_vec(wrd)\n",
    "        vectors_words.append(wrd_vector)\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "    # t-SNE reduction\n",
    "    Y = (TSNE(n_components=2, random_state=0, perplexity=15, init=\"pca\")\n",
    "        .fit_transform(vectors_words))\n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({\"x\": [x for x in Y[:, 0]],\n",
    "                    \"y\": [y for y in Y[:, 1]],\n",
    "                    \"words\": word_labels,\n",
    "                    \"color\": color_list})\n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                    x=\"x\",\n",
    "                    y=\"y\",\n",
    "                    fit_reg=False,\n",
    "                    marker=\"o\",\n",
    "                    scatter_kws={\"s\": 40,\n",
    "                                \"facecolors\": df[\"color\"]}\n",
    "    )\n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "        p1.text(df[\"x\"][line],\n",
    "                df[\"y\"][line],\n",
    "                \" \" + df[\"words\"][line].title(),\n",
    "                horizontalalignment=\"left\",\n",
    "                verticalalignment=\"bottom\", size=\"medium\",\n",
    "                color=df[\"color\"][line],\n",
    "                weight=\"normal\"\n",
    "        ).set_size(15)\n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))\n",
    "```\n",
    "\n",
    "Далее рассмотрим, как далеко располагается слово “история” и слова «неделя», «россия», «зима»:\n",
    "\n",
    "```python\n",
    "tsne_scatterplot(w2v_model, \"история\", [\"неделя\", \"россия\", \"зима\"])\n",
    "```\n",
    "Рисунок ниже показывает результат уменьшения размерности. Как видим, около истории находятся слова, связанные с учебой (“педагогика”, “теорема”), а вот “неделя”, которую мы передали как элемент списка, находится далеко от “истории”.\n",
    "\n",
    "![](https://python-school.ru/wp-content/uploads/2020/07/2_.png)\n",
    "\n",
    "*Слова на системе координат*\n",
    "\n",
    "Мы переобучили модель, увеличив число эпох обучения (аргумент epoch в методе `train`) в два раза, и построили через ту же самую функцию систему координат:\n",
    "\n",
    "![](https://python-school.ru/wp-content/uploads/2020/07/2_more_epochs_.png)\n",
    "\n",
    "*Слова на системе координат после увеличения числа эпох в два раза*\n",
    "\n",
    "Результаты оправдали себя, с “историей” стало ассоциироваться больше слов, связанных с учебой и приблизилась к нему “неделя”. Machine Learning требует постоянного экспериментирования, поэтому важно всячески настраивать и перенастраивать свою модель.\n",
    "\n",
    "***\n",
    "\n",
    "Еще больше подробностей о [NLP](https://python-school.ru/wiki/nlp/)-моделях и особенностях их обучения на реальных примерах Data Science с помощью средств языка Python, вы узнаете на наших курсах в лицензированном учебном центре обучения и повышения квалификации ИТ-специалистов в Москве.\n",
    "\n",
    "***\n",
    "\n",
    "## Источники\n",
    "\n",
    "1. Рубцова Ю. Автоматическое построение и анализ корпуса коротких текстов (постов микроблогов) для задачи разработки и тренировки тонового классификатора //Инженерия знаний и технологии семантического веба. – 2012. – Т. 1. – С. 109-116.\n",
    "2. [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)\n",
    "3. [https://en.wikipedia.org/wiki/Cosine_similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "4. [https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
    "5. [https://ru.wikipedia.org/wiki/Метод_главных_компонент](https://ru.wikipedia.org/wiki/Метод_главных_компонент)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a3366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack_ai_env",
   "language": "python",
   "name": "hack_ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
